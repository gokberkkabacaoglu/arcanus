\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[section]{placeins}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\usepackage{listings}
\usepackage{amssymb}

\title{\bf{CSE397: Assignment \#2}}
\author{Nicholas Malaya \\ Institute for Computational Engineering and Sciences \\ University of Texas at Austin} \date{}

\begin{document}
\maketitle

\newpage
\section{Problem 1}

% \begin{figure}[!htb]
%   \includegraphics[scale=.5]{plots/data.pdf}
%   \label{fig:data}
%   \caption{The data (after applying the filter) with and without
%  normally distributed noise. } 
% \end{figure}

% After applying the new filter $k(x)$ and adding gaussian noise, the data
% used as input for the inverse problem is plotted in figure 1. 

% Notice that this filter is significant: the raw data generated after
% passing through the filter (even without statistical noise) has lost
% several features of the underlying ``true'' signal. 

% \subsection{$T_{\text{SVD}}$}


% \begin{figure}[!htb]
%         \centering
%         \begin{subfigure}[bh]{0.45\textwidth}
%                 \includegraphics[width=\textwidth]{plots/tsvd0001.pdf}
%                 \caption{$\alpha=0.0001$}
%         \end{subfigure}%
%         \begin{subfigure}[bh]{0.45\textwidth}
%                 \includegraphics[width=\textwidth]{plots/tsvd001.pdf}
%                 \caption{$\alpha=0.001$}
%         \end{subfigure}
%         \centering
%         \begin{subfigure}[bh]{0.45\textwidth}
%                 \includegraphics[width=\textwidth]{plots/tsvd01.pdf}
%                 \caption{$\alpha=0.1$}
%         \end{subfigure}%
%         \begin{subfigure}[bh]{0.45\textwidth}
%                 \includegraphics[width=\textwidth]{plots/tsvd1.pdf}
%                 \caption{$\alpha=1.0$}
%         \end{subfigure}
%         \caption{$T_{\text{SVD}}$ at varying values of $\alpha$.}
%         \label{fig:svd}
% \end{figure}

% Figure \ref{fig:svd} plots the solution to the inverse problem using
% Truncated SVD at several different values of $\alpha$, the
% regularization parameter. We can see that actually, the filter nicely
% regularizing the inverse problem even for small values of $\alpha$. It
% is only for the largest value of $\alpha$ that the parameter is obviously
% too large, at which point almost all the frequencies are filtered out,
% and the solution to the inverse problem is constant and zero. 

% As expected, none of the solutions capture any of the very fine (high
% frequency) features of the original data. 

% \subsection{L-Curve}

% % \begin{figure}[!htb]
% %   \includegraphics[scale=.6]{plots/L-curve.pdf}
% %   \caption{The L-curve} 
% %  \label{fig:lcurve}
% % \end{figure}

% Figure \ref{fig:lcurve} plots the results of the L-curve criterion. The
% red dot plotted is at 0.01. This is approximately the ``optimal'' value
% of regularization, which is generally consistent with the observations
% of the previous plots. 

\section{Problem 2}

\subsection{Show that the two problems have the same minimizers}

We can see that the problems must have the same minimizers
(e.g. $x^\star$) because $\beta$ is just a scale factor, and our solution
should be invariant to homogeneous linear mappings. 

Intuitively, if we multiply our function at each point by 10x, then 
the lowest point should remain in the same place (and likewise, for the
max). 

More formally speaking, our first order necessary conditions for a local
minimizer, $x^\star$, require that if $x^\star$ is a local minimizer of
$f_2(x)$, then $g_2(x^\star) = 0 $ at the local minimizer. 

However, $f_2(x) = \beta f_1(x)$. Therefore,  $f_2(x^\star) = \beta
f_1(x^\star)$, and $g_1(x^\star) = g_2(x^\star) = 0 $. Therefore, the
gradient is also zero at $x^\star$ for $f_1$.

\subsection{Compare Steepest Descent and Newton Directions at $x_0 \in \mathbb{R}$}

\begin{align*}
 p_k&=-g_k = -\nabla f_k\\
 p_{k,1}&=-g_{k,1}\\
 p_{k,2}&=-g_{k,2} = -\beta g_{k,1}
\end{align*}

Thus, 
\begin{equation}
p_{k,2} \ne p_{k,1}
\end{equation}
for steepest descent: the search directions
vary by a scale factor. However, using Newton's method, the first search
direction is,
\begin{align*}
 H_k p_k&=-g_k \\
 H_{k,1} p_{k,1}&=-g_{k,1}\\
\end{align*}
And the second is,
\begin{align*}
 H_{k,2} p_{k,2}&=-g_{k,2}\\
 \beta H_{k,1} p_{k,2}&=- \beta g_{k,1}\\
 H_{k,1} p_{k,2}&=-g_{k,1}
\end{align*}
And therefore, 
\begin{equation}
p_{k,2} = p_{k,1}
\end{equation}
Thus, Newton's method is insensitive to the scale of the underlying
problem. Steepest decent on the other hand, does depent on the scale of
the underlying problem, and therefore it is not possible to make a good
initial step length, $\alpha$.
\end{document}