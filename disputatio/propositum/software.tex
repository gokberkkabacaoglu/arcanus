\section{Computational Methods and Software}
\label{sec:software}

write an intro discussing what is in this section\todo{write me}

\subsection{Discretization Scheme}

To solve the Navier-Stokes equations on a computer, a
Galerkin finite element method (FEM) discretization is used, which
requires that the equations of interest, equations
\ref{eqn:ns_mom}-\ref{eqn:ns_en} be cast into the weak form. Casting
these partial differential equations into a variation formulation is
accomplished by multiplying the equations by appropriate test vector
functions and integrating over the domain, $\Omega$. This has the
form,\todo{write down weak form}  
\begin{equation}
 NS
  \label{eqn:ns_weak}
\end{equation}

Some of the simulations presented here were conducted under steady
conditions, for which the $\frac{\partial}{\partial t}$ terms vanish. 
All of the simulations discussed in this work were accomplished using
linear basis functions for both the velocity and pressure.
Typically, the use of equal order elements for velocity and pressure is
ruled out in the standard Galerkin FEM formulation by the Babuska-Brezzi
condition\cite{bb-cond}. The numerical scheme used in this work is
stable with equal-order elements for velocity and pressure through the
introduction of streamline upwind/Petrov-Galerkin (SUPG) stabilization
terms as first described by Hughes\cite{Hughes198685,supg} and extended
to natural convection as in Becker and Braack\cite{Becker2002428}. The
stabilization terms add artificial dissipation that approaches zero in
the limit of smooth solutions. This scheme is ``consistent'' because the
underlying order of convergence of the numerical method is not 
affected\ref{hughes2000finite}. 

least-squares 
 ($\tau$) have the flavor\todo{indicate stabiliziation used}

This system of ODEs are discretized in time using the 
backward Euler method\cite{moin2010fundamentals}. The time interval
$(0,T)$ is sliced into $N_t$ steps of 
uniform temporal length,  $\Delta t$, where $n = 0,\dots,N_t$. 
This has the form, 
\begin{equation}
 y_{n+1} = y_n + \Delta t \, f(y_{n+1},t_{n+1}).
\end{equation}
As $f$ is non-linear, a Newtonâ€“Raphson method is used to solve the
resulting implicit nonlinear problem. While an iterative method is
significantly more computationally expensive per timestep than a similar
explicit method, the method was selected due to its unconditional
stability and ease of statistical sampling for a uniform timestep. 

\subsection{Mesh Discretization}
%
% what about mesh...
%
The meshes are scaled by system
diameter. The same number of grid points are used for every
simulation, with the total domain extents scaled up with system size. In
this way the ratio of the domain diameter to system diameter remains
fixed. Likewise, the diffusivities are proportionally scaled with grid
size to ensure that the cell Reynolds number,
\begin{equation}
 \text{Re}_\text{cell} = \frac{\text{max}(\Delta x,\Delta y) u}{\nu_T}
\end{equation}
 is maintained for
every simulation. After operation, solutions are evaluated to ensure
that the qualitative character of the solution does not
change.
%
% gave not completely described numerical methods
% for instance, have not indicated the stabilization schemes
% do not need complete equations, but should permit someone to access 
% the literature and construct precisely the numerical formulations used
%

\subsection{Simulation Geometry and Boundary Conditions}
\label{sec:bc}

\textbf{Computational Domain} 

All simulations are performed in a cuboid domain, with six
faces. We\todo{move section}
use a uniform mesh in the lateral directions, and a non-uniform
mesh\todo{makes no sense}
in height to resolve the boundary layer. \todo{sizes relative to device
typical for wind and no wind}

\textbf{Wall Boundary Conditions} 

For the ``thermal-only'' case study (no mean wind), 
periodic boundary conditions are used on the four sides, a modified
neumann condition\cite{gunzburger1989finite} on the top boundary, and
dirichlet boundary conditions on the bottom (the ground).\todo{write
down the math} These are shown schematically in Figure
\ref{fig:thermalbc}. On the ground, a ``no-slip'' velocity boundary
condition is imposed on the velocity field, and a dirichlet condition
uniformly fixes the temperature of the surface. 

\textbf{Inflow Boundary Conditions} 

text

\textbf{Mixed inflow/outflow Boundary Conditions} 

The modified neumann condition is necessary due to the presence of both
inflow and outflow across the face. In the region approximately above
the vanes, the concentrated hot plume is lifted by buoyancy
upward and out of the simulation domain. However, the radial inflow
towards the apparatus is drawn in by large scale convection cells larger
than the system diameter. Thus, our boundary conditions must permit
inflow along the areas above and external to the vanes. To avoid an
ill-posed problem, the ``v'' and ``u'' components of inflow velocity are
set to zero. \todo{explain cell reynolds}

Finally, a ``sponge layer'' is labeled near the top boundary.\todo{need
3d image} This layer artificially increases the momentum diffusivity by
up to ten times the nominal value. This was designed in response to
instabilities in the modified 
neumann boundary condition that occurred when small, high velocity fluid
parsels would exit the top. This would create high velocity inflows, and
the feedback loop would result in numerical blow-up. Mindful of the fact
that the character of solution not important in this region, and that
our physical interest remains focused on the region inside and
in immediate proximity to the vanes, we introduced a higher diffusivity
``sponge'' region that would diffuse the high velocity exiting jets
sufficiently to prevent numerically un-physical behavior. No results are
quoted from this ``sacrificial'' region, as it is not considered
meaningful. These regions are referred to by many names in the
literature\cite{doi:10.1146/annurev.fluid.36.050802.121930}, such as
absorbing layers, fringe regions, buffer zones, sponges, etc.

\begin{figure}[!htb]
  \begin{center}
    \includegraphics[width = 8 cm]{figs/thermal_only}
    \caption{Boundary conditions for the thermal-only scenario. }
    \label{fig:thermalbc}
  \end{center}
\end{figure}

The wind cases are diagrammed in figures \ref{fig:windstream} and
\ref{fig:windspan}. The wind case has a proscribed inlet boundary layer
for both the temperature profile as well as the velocity. The velocity
boundary layer is set to the common 7th power function for a
turbulent boundary layer,  
\begin{equation*}
  u_{in}(z) = U \text{ min }\left(\left(\frac{z}{\delta}\right)^7,1\right)
\end{equation*}
where $\delta$, the boundary layer thickness, is set based on data
measured by our experimental partners in the field. 
The thermal boundary layer is assumed to have a similar boundary layer,
but, as observed in real atmospheric flows, there remains a vertical
temperature gradient outside the thin boundary layer. Based on
literature a $2/3$ Kelvin per meter gradient has been
selected\cite{Blocken2007238}. The sides, outflow and top are all set to
modified neumann boundary conditions, as described above. The outflow
region in the back also needs the modified neumann as it does
occasionally exhibit mild inflow on account of the unsteady wake. Sponge
layers are set along the top and back (outflow) of the box. 

%
%335+18*tanh(-z/0.1)-z*2/3
%
\begin{figure}[!htb]
  \begin{center}
    \includegraphics[width = 8 cm]{figs/wind_streamwise}
    \caption{Boundary conditions for the wind and thermal scenario, in
   the streamwise direction.} 
    \label{fig:windstream}
  \end{center}
\end{figure}

\begin{figure}[!htb]
  \begin{center}
    \includegraphics[width = 8 cm]{figs/wind_spanwise}
    \caption{Boundary conditions for the wind and thermal scenario, in
   the spanwise direction. } 
    \label{fig:windspan}
  \end{center}
\end{figure}


\subsection{Software Stack}

The numerical approximations described above were implemented using the
GRINS library\cite{GRINSpaper} in Libmesh\cite{libMeshPaper}. 
It was designed to support multiphysics FEM
applications, the reusability and extensibility of mathematical
modeling kernels, supporting interfaces to existing solver and
discretization libraries to enable modern solution strategies, while, at
the same time, retaining flexibility to effectively address a wide range
of science or engineering problems. 

GRINS provides a platform that enables powerful numerical algorithms
such as adjoint-based AMR, adaptive modeling, sensitivity analysis,
and, eventually, enabling uncertainty quantification. While few of these
capabilities are in use for the present work, they could be useful in
future investigations. 

GRINS stands for, ``General Reacting Incompressible Navier-Stokes'',
which roughly encapsulates the physical regimes it was originally
designed to simulate. GRINS is open-source, and available on
\hyperref[www.github.com/grinsfem/grins]{github}. It is released 
under LGPL2.1.  GRINS is heavily unit tested, with over 60 tests
available to ensure the reliability of results regardless of install platform.

%The remainder of this subsection is devoted to
%discussing the underlying libraries used and the description of the
%GRINS framework.  
% PETSC\cite{petsc} trilinos\cite{trilinos}

% GRINS also uses the fparser\cite{fparser}
% library to support both parsing and compilation of mathematical
% functions into high 
% performance kernels. This capability allows for easy specification of
% boundary conditions, initial conditions, or constitutive equations from an input file. 

% Currently, libMesh has been scaled tens of thousands of cores and has
% been run on over 100,000 cores on the BG/Q machine Mira at Argonne National
% Lab\cite{libmesh-scaling}

%In principle, alternative software libraries/frameworks such as
%FEniCS\cite{fenics}, OpenFOAM\cite{openfoam}, etc. would likely be
%capable of simulating this regime. 


%
% INCLUDE IN THESIS
%
% \subsection{Solver Options}

% GRINS uses PETSC\cite{petsc} and trilinos\cite{trilinos} for numerical
% linear algebra, such as constructing and using sparse matrices, finding
% the iterative solution of linear systems, and for preconditioning.  

% While a variety of solver options have been tested in PETSC, all the
% results shown in this document utilize GMRES with block Jacobi for
% preconditioning\cite{Saad:2003} for the linear solve. 

% This uses the inverse of the diagonal block for that processor for
% preconditioning. 

% the preconditioner it's going to use to precondition the linear system
% for the solution of the diagonal block. To approximate this, incomplete
% LU factorization is used. 

% ILU(0) factorization



%% (11:41:54 AM) nick: ``-ksp_view -ksp_type gmres -pc_type bjacobi -sub_pc_type ilu -sub_pc_factor_levels 0''
%% (11:42:00 AM) Paul Bauman: OK
%% (11:42:17 AM) Paul Bauman: -pc_type is the preconditioner for the entire linear system
%% (11:42:25 AM) Paul Bauman: You're doing bjacobi = block Jacobi
%% (11:42:28 AM) nick: right
%% (11:42:36 AM) nick: and does anyone have a good reference I can learn this better? i feel as if I cant look this up, for some reason
%% (11:42:39 AM) Paul Bauman: That is just using the inverse of the diagonal block for that processor
%% (11:42:46 AM) Paul Bauman: Now
%% (11:42:55 AM) Paul Bauman: that is a linear solve
%% (11:43:17 AM) Paul Bauman: So you can use all the linear solver technology to solve or approximately that block
%% (11:43:24 AM) Paul Bauman: Hence, -sub_pc_type
%% (11:43:32 AM) hil left the room.
%% (11:43:46 AM) Paul Bauman: That's the preconditioner it's going to use to precondition the linear system for the solution of the diagonal block
%% (11:43:53 AM) Paul Bauman: You're telling it to use incomplete lu
%% (11:44:26 AM) Paul Bauman: Now the -sub_pc_factor_levels options applies to ilu
%% (11:44:28 AM) hil entered the room.
%% (11:44:44 AM) Paul Bauman: The incomplete part of imcomplete LU is about the level of fill you use
%% (11:45:05 AM) Paul Bauman: The more levels of fill you have, the more ``complete'' the incomplete LU will be
%% (11:45:08 AM) Paul Bauman: Does that make sense?
%% (11:45:23 AM) nick: no, that is where you lost me
%% (11:45:39 AM) nick: i dont think i know this level of fill
%% (11:46:17 AM) Paul Bauman: Check out Youssef Saad's book if more curious about the subject
%% (11:46:37 AM) nick: cool thanks
%% (11:46:38 AM) Paul Bauman: Suffice it to say, you heopfully shouldn't ever need to go past 3 or 4 levels of fill
%% (11:47:03 AM) Paul Bauman: Also, if you've got superlu installed with the PETSc, consider using -sub_pc_factor_mat_solver_package superlu
%% (11:47:15 AM) Paul Bauman: That's a *much* faster/better implementation than PETSc's




\subsection{Tool Chain and Simulation Custodianship}

Runs are queued on Texas Advanced Computing Center (TACC)
supercomputer's Lonestar Four and Stampede. Run durations are typically  
twelve hours to perform several hundred timesteps. 
These runs are generally submitted to the production queue and are  
264-528 processing cores, 
or 22-44 nodes on Lonestar (with 12 cores per node), and a similar number
for Stampede. The runs typically have several million degrees of freedom (DoF), 
and the local number of DoF per core is maintained at $O(10^4)$. This was
selected due to memory considerations, after a strong scaling
analysis of the performance of the code on these resources, and
after consulting with the software developers.  

After a run terminates, several scripts are automatically invoked. 
These scripts archive the run (outside of the volatile /scratch 
production directories) and simultaneously, label the concluded run with
unique metadata that defines the system environment, the jobs input
files and run definitions, as well as information detailing the
hypothesis or physics the job was intended to investigate. Finally, once
a week a script performs \textbf{rsync} on the entire archived database to
ensure more than single redundancy for the runs. 

In other words, the workflow is sufficiently advanced as to permit
rapidly queuing a series of runs (in parallel) to
investigate a variety of conditions or scenario parameters. This
capability is necessary for the optimization campaign detailed in
\ref{sec:proposed_work}, where running many concurrent investigations
will be required to adequately sample the configuration space. 
