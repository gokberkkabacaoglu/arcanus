\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[section]{placeins}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\usepackage{listings}

\title{\bf{CSE397: Group Assignment \#3}}
\author{Benjamin Crestel, Nicholas Malaya, Prabhakar Marepalli, Ajay Vadakkepatt}

 %\\ Institute for Computational Engineering and Sciences \\ University of Texas at Austin} 
\date{}

\begin{document}
\maketitle

\newpage
\section*{Problem Statement}

\section{Selection of calibration}

\section{Independent Samples}

\section{Posterior of the observations}
In the general case, the posterior distribution of the observations~$y$ can be marginalized over the parameters~$\theta$ and the true value of the observed quantity~$\tilde{y}$,
\begin{align*}
p( y | D, X)  = \int_{\tilde{y}} \int_\theta p(y, \tilde{y}, \theta | D, X) \, d\theta d\tilde{y} .
\end{align*}
We can then apply Bayes' theorem twice to get 
\begin{align*}
p( y | D, X) & = \int_{\tilde{y}} \int_\theta p(y | \tilde{y}, \theta , D, X) p(\tilde{y}, \theta | D, X) \, d\theta d\tilde{y}  \\
 & = \int_{\tilde{y}} \int_\theta p(y | \tilde{y}, \theta , D, X) p(\tilde{y} | \theta , D, X) p(\theta | D, X) \, d\theta d\tilde{y} .
\end{align*}
We can now simplify this expression a bit more by noticing that the first two conditional probability density functions~(pdf) do not depend on the observed quantities~$D$.
We end up with the expression found in the problem statement
\begin{equation} \label{eq:postobsfull}
 p( y | D, X)  = \int_{\tilde{y}} \int_\theta p(y | \tilde{y}, \theta , X) p(\tilde{y} | \theta , X) p(\theta | D, X) \, d\theta d\tilde{y} . 
\end{equation}
In this exercise, we did not assume any model inadequacy.
Therefore, the second pdf inside the integral is simply given by the deterministic physical model, i.e.
\[ p( \tilde{y} | \theta, X) = \delta(\tilde{y} - Y(\theta)) , \]
where $Y$ is the deterministic physical model.
Then expression~\ref{eq:postobsfull} simplifies more to
\begin{equation} \label{eq:postobs}
 p( y | D, X)  = \int_{\theta} p(y | Y(\theta), \theta , X) p(\theta | D, X) \, d\theta ,
\end{equation}
where $p(y | Y(\theta), \theta , X)$ is the observational model and $p(\theta | D, X)$ is the posterior distribution coming from the calibration.

Applying a Monte-Carlo approximation we evaluate integral~(\ref{eq:postobs}) thanks to the samples drawn in question~2. This gives
\begin{equation} \label{eq:postobsmc} 
p(y | D, X) \approx \frac1N \sum_{i=1}^N p(y | Y(\theta_i), \theta_i, X ) . 
\end{equation}
The variable~$y$ represents the observation and could be as such understood as a vector of all the observations made. 
However multi-dimensional model validation is cumbersome and instead we'll treat each observation separately.
For each ball, we had the results from two experiments. Let's number these experiments and call the observations $y^1(t), y^2(t), y^3(t), y^4(t)$.
In each experiment, the ball position was only recorded at a discrete number of times, we in fact we are dealing with the data
\[ y^1(t^1_1), y^1(t^1_2), \ldots, y^1(t^1_{N_1}), y^2(t^2_1), \ldots, y^2(t^2_{N_2}), \ldots, y^4(t^4_{N_4}) . \]
And therefore for any $i = 1, \ldots, 4$ and $j = 1, \ldots , N_i$, let $y^{i,j} := y^i(t^i_j)$.
We then want to evaluate
\[ p(y^{i,j} | D, X) \approx \frac1N \sum_{s=1}^N p(y^{i,j} | Y(\theta_s), \theta_s, X ) . \]
We are going to evaluate that sum on a grid.
For storage efficiency, each observation will have its own grid centered around a reasonable value (to be defined later).
The grid is then defined as
\[ y^{i,j}_k= y^{i,j}_c + k \Delta y, \quad \forall k \in \{ -m^{i,j}, m^{i,j} \} . \]
For a given $i = 1, \ldots, 4$ and $j = 1, \ldots, N_i$, we are going to store the values of
\[ \frac1N \sum_{s=1}^N p(y^i(t^i_j) = y^{i,j}_k | Y(\theta_s), \theta_s, X ), \quad \forall k \in \{-m^{i,j}, m^{i,j}\} . \]
We can now write down explicitly the formula for that quantity.
In the group project~\#2, we defined the measurement error as a multivariate Gaussian with mean and covariance depending on the calibration parameters~$\alpha$ and $\sigma$.
But here we only care about the marginalized distribution of a single observation.
Fortunately, marginalized distributions of Gaussian are also Gaussian with mean and covariance being given by the restriction of the multivariate mean and covariance to the variables of interest.
In our case, we end up with a Gaussian of mean $\alpha t^i_j$ and covariance $\sigma^2$.
Let's call $y^{i,j}_c$ the computed solution of the ball drop without noise.
We can therefore write
\[ y^{i,j} = y^{i,j}_c + \varepsilon , \]
with $\varepsilon \sim \mathcal{N}(\alpha t^i_j, \sigma^2)$.
Then we store at each grid point
\[ \frac1N \frac1{\sqrt{2\pi} \sigma} \sum_{s=1}^N  e^{-(y^{i,j}_k - y^{i,j}_c - \alpha t^i_j)^2/2\sigma^2}  , \quad \forall k \in \{-m^{i,j}, m^{i,j}\} . \]


\end{document}
