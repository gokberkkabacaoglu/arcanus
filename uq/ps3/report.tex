\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[section]{placeins}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\usepackage{listings}

\title{\bf{CSE397: Group Assignment \#3}}
\author{Benjamin Crestel, Nicholas Malaya, Prabhakar Marepalli, Ajay Vadakkepatt}

 %\\ Institute for Computational Engineering and Sciences \\ University of Texas at Austin} 
\date{}

\begin{document}
\maketitle

\newpage
\section*{Problem Statement}

\section{Selection of calibration}

\section{Independent Samples}

\section{Posterior of the observations}
In the general case, the posterior distribution of the observations~$y$ can be marginalized over the parameters~$\theta$ and the true value of the observed quantity~$\tilde{y}$,
\begin{align*}
p( y | D, X)  = \int_{\tilde{y}} \int_\theta p(y, \tilde{y}, \theta | D, X) \, d\theta d\tilde{y} .
\end{align*}
We can then apply Bayes' theorem twice to get 
\begin{align*}
p( y | D, X) & = \int_{\tilde{y}} \int_\theta p(y | \tilde{y}, \theta , D, X) p(\tilde{y}, \theta | D, X) \, d\theta d\tilde{y}  \\
 & = \int_{\tilde{y}} \int_\theta p(y | \tilde{y}, \theta , D, X) p(\tilde{y} | \theta , D, X) p(\theta | D, X) \, d\theta d\tilde{y} .
\end{align*}
We can now simplify this expression a bit more by noticing that the first two conditional probability density functions~(pdf) do not depend on the observed quantities~$D$.
We end up with the expression found in the problem statement
\begin{equation} \label{eq:postobsfull}
 p( y | D, X)  = \int_{\tilde{y}} \int_\theta p(y | \tilde{y}, \theta , X) p(\tilde{y} | \theta , X) p(\theta | D, X) \, d\theta d\tilde{y} . 
\end{equation}
In this exercise, we did not assume any model inadequacy.
Therefore, the second pdf inside the integral is simply given by the deterministic physical model, i.e.
\[ p( \tilde{y} | \theta, X) = \delta(\tilde{y} - Y(\theta)) , \]
where $Y$ is the deterministic physical model.
Then expression~\ref{eq:postobsfull} simplifies more to
\begin{equation} \label{eq:postobs}
 p( y | D, X)  = \int_{\theta} p(y | Y(\theta), \theta , X) p(\theta | D, X) \, d\theta ,
\end{equation}
where $p(y | Y(\theta), \theta , X)$ is the observational model and $p(\theta | D, X)$ is the posterior distribution coming from the calibration.

Applying a Monte-Carlo approximation we evaluate integral~(\ref{eq:postobs}) thanks to the samples drawn in question~2. This gives
\begin{equation} \label{eq:postobsmc} 
p(y | D, X) \approx \frac1N \sum_{i=1}^N p(y | Y(\theta_i), \theta_i, X ) . 
\end{equation}
The variable~$y$ represents the observation and could be as such understood as a vector of all the observations made. 
However multi-dimensional model validation is cumbersome and instead we'll treat each observation separately.
For each ball, we had the results from two experiments. Let's number these experiments and call the observations $y^1(t), y^2(t), y^3(t), y^4(t)$.
In each experiment, the ball position was only recorded at a discrete number of times, we in fact we are dealing with the data
\[ y^1(t^1_1), y^1(t^1_2), \ldots, y^1(t^1_{N_1}), y^2(t^2_1), \ldots, y^2(t^2_{N_2}), \ldots, y^4(t^4_{N_4}) . \]
And therefore for any $i = 1, \ldots, 4$ and $j = 1, \ldots , N_i$, we want to evaluate
\[ p(y^i(t^i_j) | D, X) \approx \frac1N \sum_{s=1}^N p(y^i(t^i_j) | Y(\theta_s), \theta_s, X ) . \]
Because we do not know the probabilities analytically, we are going to evaluate that sum on a grid.
For storage efficiency, each observation will have its own grid centered around a reasonable value (to be defined later).
The grid is then defined as
\[ y^{i,j}_k= y^{i,j}_c + k \Delta y, \quad \forall k \in \{ -m^{i,j}, m^{i,j} \} . \]
For a given $i = 1, \ldots, 4$ and $j = 1, \ldots, N_i$, we are going to store the values of
\[ \frac1N \sum_{s=1}^N p(y^i(t^i_j) = y^{i,j}_k | Y(\theta_s), \theta_s, X ), \quad \forall k \in \{-m^{i,j}, m^{i,j}\} . \]

\end{document}
